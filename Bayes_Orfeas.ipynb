{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "---\n",
    "\n",
    "You can find below the code that was used to generate the activity of place cells on a linear track.\n",
    "Use the code and the decoding procedure you lerned about in the lesson to explore how different characteristic of the data impact our ability to decode position.\n",
    "In particular:\n",
    "\n",
    "A - Try to use different fraction of our data samples. How does the median error change when the the number of available sample gets larger? You do not need to re-generate any data, just randomly sub-sample the data to different fractions.\n",
    "\n",
    "B - How many place cells do we need to reliably decode? Try to re-do the decoding using only 10 cell, then 20, and so on. How does the median error change? Does it reach an asymptote? (Also in this case, you do not need to re-generate the data, you can just select a random subset of cells each time)\n",
    "\n",
    "C - Generate new data using the code below, changing the firing rate noise (changing the value of the variable `noise firing_rate`). How does this noise impact the decoding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "track_length = 200. # the length of our linear track (eg in centimeter)\n",
    "average_firing_rate = 5 # the peak firing rate, averaged across the population \n",
    "n_cells = 100 # how many cells we are recording\n",
    "pf_centers = np.random.rand(n_cells) * track_length # the centers of the place fields for all cells drawn randomly with a uniform distribution on the track\n",
    "pf_size = np.random.gamma(10, size=n_cells) # the size (width) of the place fields, drawn randomly from a gamma distribution \n",
    "pf_rate = np.random.exponential(scale=average_firing_rate, size=n_cells) # the peak firing rate for each cell, drawn from an exponential distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0., 200.)\n",
    "true_firing_rate_maps = np.zeros((n_cells, len(bins)))\n",
    "for i in range(n_cells):\n",
    "    true_firing_rate_maps[i,:] = pf_rate[i] * np.exp(-((bins-pf_centers[i])**2)/(2*pf_size[i]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE TRAJECTORY\n",
    "\n",
    "n_runs = 10\n",
    "use_stops = False\n",
    "av_running_speed = 10 # the average running speed (in cm/s)\n",
    "fps = 10 # the number of \"video frames\" per second \n",
    "running_speed_a = np.random.chisquare(10, size=n_runs) # running speed in the two directions\n",
    "running_speed_b = np.random.chisquare(10, size=n_runs) \n",
    "\n",
    "stopping_time_a = np.random.chisquare(15, size=n_runs) # the time the mouse will spend at the two ends of the track\n",
    "stopping_time_b = np.random.chisquare(15, size=n_runs)\n",
    "\n",
    "x = np.array([])\n",
    "\n",
    "\n",
    "for i in range(n_runs):\n",
    "    stop1 = np.ones((int(stopping_time_a[i]*fps),)) * 0.\n",
    "    run_length = len(bins) * fps / running_speed_a[i]\n",
    "    run1 = np.linspace(0., float(len(bins)-1), int(run_length))\n",
    "    stop2 = np.ones((int(stopping_time_b[i]*fps),)) * (len(bins)-1.)\n",
    "    run_length = len(bins) * fps / running_speed_b[i]\n",
    "    run2 = np.linspace(len(bins)-1., 0., int(run_length))\n",
    "    if use_stops:\n",
    "        x = np.concatenate((x, stop1, run1, stop2, run2))\n",
    "    else:\n",
    "         x = np.concatenate((x, run1, run2))\n",
    "t = np.arange(len(x))/fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 10000.\n",
    "t_sampling = np.arange(0, t[-1], 1. / sampling_rate)\n",
    "x_sampling = np.floor(np.interp(t_sampling, t, x))\n",
    "noise_firing_rate = 0.1 # the baseline noise firing rate =0.1\n",
    "#noise_firing_rate = 1\n",
    "spikes = []\n",
    "\n",
    "for i in range(n_cells):\n",
    "    inst_rate = true_firing_rate_maps[i,x_sampling.astype(np.int32)] + noise_firing_rate\n",
    "    spikes_loc = np.random.poisson(inst_rate/sampling_rate)\n",
    "    sp = np.argwhere(spikes_loc)\n",
    "    t_sp = t_sampling[sp]\n",
    "    spikes.append(t_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'linear_track_data.pickle' # change this name when you save new data\n",
    "#file_name = 'linear_track_data.picklenoise1' \n",
    "\n",
    "\n",
    "out_data = {}\n",
    "out_data['x'] = x\n",
    "out_data['t'] = t\n",
    "out_data['spikes'] = spikes\n",
    "out_data['track_length'] = track_length\n",
    "out_data['fps'] = fps\n",
    "\n",
    "with open('data/'+file_name,'wb') as f:\n",
    "    pickle.dump(out_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spikes is a list of 100 slots, as many as the n_cells, so 1% is 1, 5% is 5 etc. \n",
    "spikes1=spikes[90]\n",
    "spikes5=spikes[37:42]\n",
    "spikes10=spikes[79:89]\n",
    "spikes20=spikes[80:100]\n",
    "spikes50=spikes[50:]\n",
    "spikes5050=spikes[:50]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we compute the poistion at which each spike was emitted\n",
    "spike_positions = [np.interp(s, t, x) for s in spikes]\n",
    "\n",
    "spike_positions1 = [np.interp(s, t, x) for s in spikes1]\n",
    "spike_positions5 = [np.interp(s, t, x) for s in spikes5]\n",
    "spike_positions10 = [np.interp(s, t, x) for s in spikes10]\n",
    "spike_positions20 = [np.interp(s, t, x) for s in spikes20]\n",
    "spike_positions50 = [np.interp(s, t, x) for s in spikes50]\n",
    "spike_positions5050 = [np.interp(s, t, x) for s in spikes5050]\n",
    "\n",
    "\n",
    "space_bins = np.arange(0., track_length, 5.) # binnin in bins of 5 cms\n",
    "\n",
    "# we compute histograms for each cell\n",
    "spikes_hist= [np.histogram(s, space_bins)[0] for s in spike_positions]\n",
    "spikes_hist = np.asarray(spikes_hist)\n",
    "\n",
    "spikes_hist1= [np.histogram(s, space_bins)[0] for s in spike_positions1]\n",
    "spikes_hist1 = np.asarray(spikes_hist1)\n",
    "spikes_hist5= [np.histogram(s, space_bins)[0] for s in spike_positions5]\n",
    "spikes_hist5 = np.asarray(spikes_hist5)\n",
    "spikes_hist10= [np.histogram(s, space_bins)[0] for s in spike_positions10]\n",
    "spikes_hist10 = np.asarray(spikes_hist10)\n",
    "spikes_hist20= [np.histogram(s, space_bins)[0] for s in spike_positions20]\n",
    "spikes_hist20 = np.asarray(spikes_hist20)\n",
    "spikes_hist50= [np.histogram(s, space_bins)[0] for s in spike_positions50]\n",
    "spikes_hist50 = np.asarray(spikes_hist50)\n",
    "spikes_hist5050= [np.histogram(s, space_bins)[0] for s in spike_positions5050]\n",
    "spikes_hist5050 = np.asarray(spikes_hist5050)\n",
    "\n",
    "# we also need an \"occupancy histogram\" in order to normalize the firing rates maps \n",
    "occupancy = np.histogram(x, space_bins)[0] /  fps\n",
    "\n",
    "firing_rate_maps = spikes_hist / occupancy \n",
    "\n",
    "firing_rate_maps1 = spikes_hist1 / occupancy \n",
    "firing_rate_maps5 = spikes_hist5 / occupancy\n",
    "firing_rate_maps10 = spikes_hist10 / occupancy \n",
    "firing_rate_maps20 = spikes_hist20 / occupancy \n",
    "firing_rate_maps50 = spikes_hist50 / occupancy\n",
    "firing_rate_maps5050 = spikes_hist5050 / occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_count= [np.histogram(s,t)[0] for s in spikes]\n",
    "spikes_count = np.asarray(spikes_count).T # we transpose the matrix to have the more familiar samples x features shape\n",
    "\n",
    "spikes_count1= [np.histogram(s,t)[0] for s in spikes1]\n",
    "spikes_count1 = np.asarray(spikes_count1).T\n",
    "spikes_count5= [np.histogram(s,t)[0] for s in spikes5]\n",
    "spikes_count5 = np.asarray(spikes_count5).T\n",
    "spikes_count10= [np.histogram(s,t)[0] for s in spikes10]\n",
    "spikes_count10= np.asarray(spikes_count10).T\n",
    "spikes_count20= [np.histogram(s,t)[0] for s in spikes20]\n",
    "spikes_count20 = np.asarray(spikes_count20).T\n",
    "spikes_count50= [np.histogram(s,t)[0] for s in spikes50]\n",
    "spikes_count50= np.asarray(spikes_count50).T\n",
    "spikes_count5050= [np.histogram(s,t)[0] for s in spikes5050]\n",
    "spikes_count5050= np.asarray(spikes_count5050).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median error: 4.402234636871498 cm\n",
      "Median error1%: 98.66386554621849 cm\n",
      "Median error5%: 23.70891089108909 cm\n",
      "Median error10%: 26.74083129584352 cm\n",
      "Median error20%: 18.022255192878333 cm\n",
      "Median error last 50%: 7.009779951100242 cm\n",
      "Median error first 50%: 5.992665036674815 cm\n"
     ]
    }
   ],
   "source": [
    "#prior = occupancy / sum(occupancy)\n",
    "true_x = x[:-1] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoding_times = t[:-1]\n",
    "epsilon = pow(1,-10)\n",
    "log_posteriors = spikes_count @ np.log(firing_rate_maps+epsilon) - (1./fps)*np.sum(firing_rate_maps, axis=0) #+ np.log(prior + epsilon)\n",
    "\n",
    "log_posteriors1 = spikes_count1 @ np.log(firing_rate_maps1+epsilon) - (1./fps)*np.sum(firing_rate_maps1, axis=0) #+ np.log(prior + epsilon)\n",
    "log_posteriors5 = spikes_count5 @ np.log(firing_rate_maps5+epsilon) - (1./fps)*np.sum(firing_rate_maps5, axis=0) #+ np.log(prior + epsilon)\n",
    "log_posteriors10 = spikes_count10 @ np.log(firing_rate_maps10+epsilon) - (1./fps)*np.sum(firing_rate_maps10, axis=0) #+ np.log(prior + epsilon)\n",
    "log_posteriors20 = spikes_count20 @ np.log(firing_rate_maps20+epsilon) - (1./fps)*np.sum(firing_rate_maps20, axis=0) #+ np.log(prior + epsilon)\n",
    "log_posteriors50 = spikes_count50 @ np.log(firing_rate_maps50+epsilon) - (1./fps)*np.sum(firing_rate_maps50, axis=0) #+ np.log(prior + epsilon)\n",
    "log_posteriors5050 = spikes_count5050 @ np.log(firing_rate_maps5050+epsilon) - (1./fps)*np.sum(firing_rate_maps5050, axis=0) #+ np.log(prior + epsilon)\n",
    "\n",
    "\n",
    "x_parallel = [space_bins[np.argmax(P)] for P in log_posteriors]\n",
    "\n",
    "x_parallel1 = [space_bins[np.argmax(P)] for P in log_posteriors1]\n",
    "x_parallel5 = [space_bins[np.argmax(P)] for P in log_posteriors5]\n",
    "x_parallel10 = [space_bins[np.argmax(P)] for P in log_posteriors10]\n",
    "x_parallel20 = [space_bins[np.argmax(P)] for P in log_posteriors20]\n",
    "x_parallel50 = [space_bins[np.argmax(P)] for P in log_posteriors50]\n",
    "x_parallel5050 = [space_bins[np.argmax(P)] for P in log_posteriors5050]\n",
    "\n",
    "\n",
    "\n",
    "# error distribution\n",
    "mse = np.sqrt((true_x-x_parallel)**2)\n",
    "\n",
    "mse1=np.sqrt((true_x-x_parallel1)**2)\n",
    "mse5=np.sqrt((true_x-x_parallel5)**2)\n",
    "mse10=np.sqrt((true_x-x_parallel10)**2)\n",
    "mse20=np.sqrt((true_x-x_parallel20)**2)\n",
    "mse50=np.sqrt((true_x-x_parallel50)**2)\n",
    "mse5050=np.sqrt((true_x-x_parallel5050)**2)\n",
    "\n",
    "\n",
    "#sns.histplot(mse10)\n",
    "#plt.axvline(x = np.nanmedian(mse),c='r')\n",
    "print(f'Median error: {np.nanmedian(mse)} cm')\n",
    "print(f'Median error1%: {np.nanmedian(mse1)} cm')\n",
    "print(f'Median error5%: {np.nanmedian(mse5)} cm')\n",
    "print(f'Median error10%: {np.nanmedian(mse10)} cm')\n",
    "print(f'Median error20%: {np.nanmedian(mse20)} cm')\n",
    "print(f'Median error last 50%: {np.nanmedian(mse50)} cm')\n",
    "print(f'Median error first 50%: {np.nanmedian(mse5050)} cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running 2 simulations with different noise levels I got these results:\n",
    "\n",
    "# Noise = 0.1\n",
    "\n",
    "#Median error: 3.88032258064516 cm\n",
    "#Median error1%: 53.33609809487978 cm\n",
    "#Median error5%: 41.604781172584644 cm\n",
    "#Median error10%: 19.184588849524516 cm\n",
    "#Median error20%: 13.366241159220294 cm\n",
    "#Median error last 50%: 5.696522024983572 cm\n",
    "#Median error first 50%: 5.986930837647691 cm\n",
    "\n",
    "#Noise = 1\n",
    "\n",
    "#Median error: 4.220312623352015 cm\n",
    "#Median error1%: 65.09205020920501 cm\n",
    "#Median error5%: 31.394949170679514 cm\n",
    "#Median error10%: 24.19741252522489 cm\n",
    "#Median error20%: 18.62509471542075 cm\n",
    "#Median error last 50%: 6.64414258188825 cm\n",
    "#Median error first 50%: 8.561682723185612 cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Our results show that with a bigger sample, we get smaller errors. However, it's been noticed that the error depends on the \"area\" of the sample,first 50% has a higher error than last 50% of the sample. If we increase the noise (by 10 times, from 0,1 to 1) we get increased ms errors in our decoding. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median error for[0]: 6.601476014760152 cm\n",
      "Median error for[1]: 6.7049180327868925 cm\n",
      "Median error for[2]: 7.167597765363126 cm\n",
      "Median error for[3]: 6.723628691983123 cm\n",
      "Median error for[4]: 6.675105485232066 cm\n",
      "Median error for[5]: 6.607920792079213 cm\n",
      "Median error for[6]: 6.541176470588255 cm\n",
      "Median error for[7]: 6.467326732673257 cm\n",
      "Median error for[8]: 6.542787286063572 cm\n",
      "Median error for[9]: 6.593147751605997 cm\n",
      "Median error for[10]: 6.7995780590717345 cm\n",
      "Median error for[11]: 6.7724288840262545 cm\n",
      "Median error for[12]: 6.783382789317528 cm\n",
      "Median error for[13]: 6.5935397039031045 cm\n",
      "Median error for[14]: 6.59299781181619 cm\n",
      "Median error for[15]: 6.606060606060609 cm\n",
      "Median error for[16]: 6.390099009900979 cm\n",
      "Median error for[17]: 6.202933985330077 cm\n",
      "Median error for[18]: 6.257065948855981 cm\n",
      "Median error for[19]: 6.7671232876712395 cm\n",
      "Median error for[20]: 7.042826552462529 cm\n",
      "Median error for[21]: 6.970588235294116 cm\n",
      "Median error for[22]: 6.7927321668909855 cm\n",
      "Median error for[23]: 6.761038961038935 cm\n",
      "Median error for[24]: 6.345895020188422 cm\n",
      "Median error for[25]: 6.275711159737398 cm\n",
      "Median error for[26]: 6.3234421364985 cm\n",
      "Median error for[27]: 6.156424581005595 cm\n",
      "Median error for[28]: 6.1899441340782175 cm\n",
      "Median error for[29]: 6.139344262295083 cm\n",
      "Median error for[30]: 6.526041666666668 cm\n",
      "Median error for[31]: 6.528189910979222 cm\n",
      "Median error for[32]: 6.601265822784811 cm\n",
      "Median error for[33]: 6.718811881188117 cm\n",
      "Median error for[34]: 6.604166666666686 cm\n",
      "Median error for[35]: 6.6291390728476856 cm\n",
      "Median error for[36]: 6.23887240356083 cm\n",
      "Median error for[37]: 6.214022140221402 cm\n",
      "Median error for[38]: 6.323529411764696 cm\n",
      "Median error for[39]: 6.197916666666657 cm\n",
      "Median error for[40]: 6.26199261992619 cm\n",
      "Median error for[41]: 6.304950495049496 cm\n",
      "Median error for[42]: 6.369003690036891 cm\n",
      "Median error for[43]: 6.368308351177745 cm\n",
      "Median error for[44]: 6.501845018450183 cm\n",
      "Median error for[45]: 6.623145400593472 cm\n",
      "Median error for[46]: 6.596026490066237 cm\n",
      "Median error for[47]: 6.568265682656829 cm\n",
      "Median error for[48]: 6.810089020771512 cm\n",
      "Median error for[49]: 7.072700296735917 cm\n",
      "Median error for[50]: 7.481188118811872 cm\n",
      "Median error for[51]: 7.425974025974 cm\n",
      "Median error for[52]: 7.350553505535046 cm\n",
      "Median error for[53]: 7.471810089020764 cm\n",
      "Median error for[54]: 7.2284866468842495 cm\n",
      "Median error for[55]: 7.296089385474886 cm\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Exceeds limit\n",
      "Median error for total: 6.598646156425524 cm\n",
      "Standard deviation of total: 0.3474918829060495\n"
     ]
    }
   ],
   "source": [
    "# How many cells to reliably decode ?\n",
    "\n",
    "# We saw that at 20% the error is still double digits and even at 50% it's still a third bigger\n",
    "# than our original error (mse~4). So we don't expect much from 10, 20 cells.\n",
    "# ! Reminder ! len(spikes)=100 so 50% is 50 cells etc. \n",
    "# I take increments of y cells and calculate the mse of y cells along the whole sample.\n",
    "mses=[]\n",
    "y=45\n",
    "\n",
    "for i in range(100):\n",
    "    spikeshm=spikes[i:i+y]\n",
    "    if i+y<101:\n",
    "        \n",
    "\n",
    "        spike_positionshm = [np.interp(s, t, x) for s in spikeshm]\n",
    "\n",
    "        spikes_histhm= [np.histogram(s, space_bins)[0] for s in spike_positionshm]\n",
    "        spikes_histhm = np.asarray(spikes_histhm)\n",
    "\n",
    "        firing_rate_mapshm = spikes_histhm / occupancy \n",
    "\n",
    "        spikes_counthm= [np.histogram(s,t)[0] for s in spikeshm]\n",
    "        spikes_counthm = np.asarray(spikes_counthm).T\n",
    "\n",
    "        log_posteriorshm = spikes_counthm @ np.log(firing_rate_mapshm+epsilon) - (1./fps)*np.sum(firing_rate_mapshm, axis=0) #+ np.log(prior + epsilon)\n",
    "        x_parallelhm = [space_bins[np.argmax(P)] for P in log_posteriorshm]\n",
    "        msehm=np.sqrt((true_x-x_parallelhm)**2)\n",
    "\n",
    "        mses.append(np.nanmedian(msehm))\n",
    "\n",
    "        print(f'Median error for[{i}]: {np.nanmedian(msehm)} cm')\n",
    "    else:\n",
    "        print('Exceeds limit')\n",
    "print(f'Median error for total: {np.nanmedian(mses)} cm')\n",
    "print(f'Standard deviation of total: {np.std(mses)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> By executing the cell above numerous times I started noticing a decreasing mse pattern even from 10 cells.However the mse and its fluctuations were still high. After 32 cells (y=32) the mse was mostly under 10 and around 45 cells the fluctuations became smaller (standard deviation). I would have to say we need at least half of the sample to have a similar decoding result as the whole sample. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec757111aa82fc412dab5a41ba1a33fdb6db5c8112df3ff06fec0dbff050b412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
